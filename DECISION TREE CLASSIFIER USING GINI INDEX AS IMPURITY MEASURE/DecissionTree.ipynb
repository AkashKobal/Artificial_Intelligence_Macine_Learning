{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsUmy_fJoT7x"
      },
      "outputs": [],
      "source": [
        "# create a pandas frame for this file and edxplore its content\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/German Credit Data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "id": "FREP_1F-pFU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "yyNQv9IJpJPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.describe)"
      ],
      "metadata": {
        "id": "NiwKsTAdpL6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "Qx5XpGnfpOuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Print the first five records and first 7 columns\n",
        "df.iloc[0:5,0:8] #[rows,columns]\n",
        "# The code \"df.iloc[0:5,0:7]\" selects a subset of data from a DataFrame called \"df\"., it selects the first 5 rows and the first 7 columns of the DataFrame. \n",
        "\n",
        "# The \"iloc\" method is used to select data based on its integer location. The first argument \"0:5\" specifies the range of rows to select, starting from row 0 and ending at row 4 (since the upper limit is not inclusive). The second argument \"0:7\" specifies the range of columns to select, starting from column 0 and ending at column 6.\n",
        "\n",
        "# This code can be useful for exploring the structure and contents of a DataFrame, or for selecting a specific subset of data for further analysis or visualization. Keep in mind that the selected subset of data may not represent the entire dataset, so it's important to consider the context and purpose of the analysis. "
      ],
      "metadata": {
        "id": "GBob-9VipR7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Print the first five records and remaining columns\n",
        "df.iloc[0:5,7:15]"
      ],
      "metadata": {
        "id": "f1Moi18Op_kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Few of the columns are categorical and are infered as objects. Ex: checkin_acc. Print all unique values of this column\n",
        "df[\"checkin_acc\"].unique()\n",
        "# The output of the code \"df[\"checkin_acc\"].unique()\" will depend on the structure and content of the pandas dataframe named \"df\". Assuming that there is a column named \"checkin_acc\" in the dataframe, this code will return an array of unique values that are present in that column. \n",
        "\n",
        "# For example, if the \"checkin_acc\" column contains categorical data related to bank accounts, such as \"no account\", \"low balance\", \"medium balance\", and \"high balance\", then the output of this code will be an array containing these unique values.\n",
        "\n",
        "# This code is useful for exploring the unique values in a column of a dataframe, which can help in identifying any data quality issues or patterns in the data. It can also be used for data preprocessing tasks such as encoding categorical variables for machine learning models. "
      ],
      "metadata": {
        "id": "xfjK2-AsqfHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Encode all categorical features using one-hot encoding. A feature with n values is encoded using (n-1) values, retaining the first one (drop_first = True)\n",
        "# First, you have a DataFrame named df, and you want to create a list of column names as X_features, excluding the column named 'status'. To accomplish this, you can use the following code:\n",
        "# Next, you want to encode the categorical features in the DataFrame df[X_features] using one-hot encoding. The resulting encoded DataFrame will be stored in encoded_df. To achieve this, you can utilize the pd.get_dummies() function with the drop_first=True parameter, which drops the first category for each feature to avoid multicollinearity:\n",
        "# Finally, you can print the list of column names in the encoded_df DataFrame using print(list(encoded_df.columns)):\n",
        "# This will display the column names of the encoded DataFrame.\n",
        "X_features = list(df.columns) #creating the list of the columns\n",
        "X_features.remove('status') #deleting the status column\n",
        "encoded_df = pd.get_dummies(df[X_features],drop_first=True) #encoding the feature using get dummies and drop the first values\n",
        "print(list(encoded_df.columns)) #printing the result"
      ],
      "metadata": {
        "id": "4KNGjsgDq3bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Make independent features of the encoded frame as X and column 'status' as dependent feature.\n",
        "X = encoded_df\n",
        "Y = df[\"status\"]"
      ],
      "metadata": {
        "id": "Hge6nfK3srAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Divide data into 70% training and 30% as testing.  \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3, random_state =42)\n",
        "\n",
        "# X and Y are the arrays or DataFrames representing your input features and target variable, respectively.\n",
        "# test_size=0.3 indicates that you want to allocate 30% of the data to the testing set. This means that 70% of the data will be allocated to the training set.\n",
        "# random_state=42 sets the random seed to 42, ensuring reproducibility of the split. This means that each time you run the code with the same random seed, you will obtain the same split.\n",
        "# After executing this code, the function will split your data into four arrays or DataFrames: X_train, Y_train, X_test, and Y_test. The training data will be stored in X_train and Y_train, while the testing data will be stored in X_test and Y_test."
      ],
      "metadata": {
        "id": "tfjQwHvxuDOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Train a decision tree model using Gini INdex and depth of 3\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(criterion = 'gini', max_depth=3 )\n",
        "clf.fit(X_train,Y_train)\n",
        "# n this code, you import the DecisionTreeClassifier class from sklearn.tree. Then, you create an instance of the classifier named clf. The classifier is initialized with the following parameters:\n",
        "\n",
        "# criterion='gini' sets the criterion used to measure the quality of a split in the decision tree. The 'gini' criterion refers to the Gini impurity, which is a measure of the node's impurity based on class probabilities. Other supported criteria are 'entropy' for the Shannon information gain and 'log_loss' for logarithmic loss [1].\n",
        "# max_depth=3 sets the maximum depth of the decision tree. This parameter limits the number of levels the tree can grow to. In this case, the tree is limited to a depth of 3, meaning it can have at most 3 levels from the root to the leaves [2].\n",
        "# Once the classifier is instantiated, you fit it to your training data using clf.fit(X_train, Y_train). This trains the decision tree classifier on the features X_train and their corresponding targets Y_train."
      ],
      "metadata": {
        "id": "0XyvS8mkuru0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Make predictions on test/validation data\n",
        "pred_y = clf.predict(X_test)"
      ],
      "metadata": {
        "id": "8yOGsvzUwQcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Print the confusion matrix, accuracy and AUC score of this model on test set\n",
        "from sklearn import metrics\n",
        "print(\"Confusion Matrix is\\n\",metrics.confusion_matrix(pred_y, y_test))\n",
        "print(\"Accuracy is\", metrics.accuracy_score(pred_y,y_test))\n",
        "print(\"AUC Score is\", metrics.roc_auc_score(pred_y, y_test))\n",
        "\n",
        "# confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
        "# computes a confusion matrix to evaluate the accuracy of a classification. \n",
        "# It compares the true labels (y_true) with the predicted labels (y_pred) and \n",
        "# provides information about the number of observations known to be in each group and \n",
        "# the number of observations predicted to be in each group\n",
        "# accuracy_score(y_true, y_pred) calculates the accuracy of the classification by comparing the true \n",
        "# labels (y_true) with the predicted labels (y_pred).\n",
        "# roc_auc_score(y_true, y_score) computes the area under the receiver operating characteristic\n",
        "# curve (ROC AUC) for binary classification tasks.\n",
        "# It measures the classifier's ability to rank the positive and negative samples correctly [1].\n",
        "# By calling these functions with pred_y as the predicted labels and y_test as the true labels, \n",
        "# you can obtain the confusion matrix, accuracy score, and ROC AUC score, respectively. \n"
      ],
      "metadata": {
        "id": "dhsPrWitxC_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Visualize the tree using grapghviz and pydotplus libraries\n",
        "from sklearn.tree import export_graphviz\n",
        "import pydotplus as pdot\n",
        "from IPython.display import Image\n",
        "export_graphviz(clf,out_file = \"tree.odt\", feature_names = X_train.columns, filled = True)\n",
        "graph = pdot.graphviz.graph_from_dot_file(\"tree.odt\")\n",
        "graph.write_jpg(\"tree.png\")\n",
        "Image(filename = \"tree.png\")\n",
        "\n",
        "# In this code, you perform the following steps:\n",
        "\n",
        "# Import the necessary modules and functions: export_graphviz from sklearn.tree, pydotplus as pdot, and Image from IPython.display.\n",
        "# Call export_graphviz to export the decision tree model (clf) to the GraphViz format.\n",
        "#  The out_file parameter specifies the output file name (\"tree.odt\" in this case), and \n",
        "#  the feature_names parameter provides the names of the features in your dataset.\n",
        "# Use graphviz.graph_from_dot_file from pydotplus to create a graph from the exported GraphViz file (\"tree.odt\").\n",
        "# Write the graph to a JPG image file (\"tree.png\") using write_jpg method of the graph object.\n",
        "# Display the image using IPython.display.Image by providing the filename of the image (\"tree.png\")."
      ],
      "metadata": {
        "id": "ds5qZDISDGks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKrVoKSNEHKZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}